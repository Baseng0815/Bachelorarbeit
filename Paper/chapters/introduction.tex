\chapter{Introduction}
\section{Notation}

\section{Block ciphers}

Securing communication channels between different parties has been a long-term
subject of study for cryptographers and engineers which is essential to our
modern world to cope with ever-increasing amounts of devices producing and
sharing data. The main way to facilitate high-throughput, confidential
communications nowadays is through the use of symmetric cryptography in which
two parties share a common secret, called a key, which allows them to encrypt,
share and subsequently decrypt messages to achieve confidentiality against
third parties. Ciphers can be divided into two categories; block ciphers, which
always encrypt fixed-sized messages called blocks, and stream ciphers, which
continuously provide encryption for an arbitrarily long, constant stream of
data.

A block cipher can be defined as a bijection between the input block (the
message) and the output block (the ciphertext). For any block cipher with block
size $n$, we denote the key-dependent encryption and decryption functions as
$E_K,D_K:\F{n}\rightarrow \F{n}$. The simplest way to
characterize this bijection is through a lookup table which yields the highest
possible performance as each block can be encrypted by one simple lookup
depending on the key and the message. This is not practical though due to most
ciphers working with block and key sizes $n,|K|\geq 64$. For a block cipher
with $n=64,|K|=128$, a space of $2^{64}2^{128}64=2^{198}$ is necessary.
Considering modern consumer hard disks being able to store data in the order of
$2^{40}$, it is easy to see that a lookup table is wholly impractical. We
therefore describe block ciphers algorithmically which opens up possibilities
for different tradeoffs and security concerns.


\subsection{GIFT}

\texttt{GIFT}\cite{gift:2017}, first presented in the \textit{CHES 2017}
cryptographic hardware and embedded systems conference, is a lightweight block
cipher based on a previous design called \texttt{PRESENT}, developed in 2007. Its
goal is to offer maximum security while being extremely light on resources.
Modern battery-powered devices like RFID tags or low-latency operations like
on-the-fly disc encryption present strong hardware and power constraints. GIFT
aims to be a simple, low-energy cipher suited for these kinds of applications.

\texttt{GIFT} comes in two variants; \verb|GIFT-64| working with 64-bit blocks
and \texttt{GIFT-128} working with 128-bit blocks. In both cases, the key is 128
bits long. The design is a very simple, round-based substitution-permutation
network (SPN). One round consists in a sequential application of the confusion
layer by means of 4-bit S-boxes and subsequent diffusion through bit
permutation. After the bit permutation, a round key is added to the cipher
state and the single round is complete. \texttt{GIFT-64} uses 28 rounds while
\texttt{GIFT-128} uses 40 rounds.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figures/GIFT-64.pdf}
    \caption{Two rounds of GIFT-64}
\end{figure}

\subsubsection{Substitution layer}

The input of \texttt{GIFT} is split into 4-bit nibbles which are then fed into
16 S-boxes for \texttt{GIFT-64} and 32 S-boxes for \texttt{GIFT-128}. The S-box
$S:\F{4}\rightarrow \F{4}$ is defined as follows:

\[
    \begin{array}{l|cccccccccccccccc}
        x & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & a & b & c & d & e & f \\
        \hline
        S(x) & 1 & a & 4 & c & 6 & f & 3 & 9 & 2 & d & b & 7 & 5 & 0 & 8 & e
    \end{array}
\]

\subsubsection{Permutation layer}

The permutation $P$ works on individual bits and maps bit $b_i$ to $b_{P(i)},
i\in\{0,1,\dots,n-1\}$. The different permutations for \texttt{GIFT-64} and
\texttt{GIFT-128} can be expressed by:

\begin{align*}
    P_{64}(i)&=4\left\lfloor\frac{i}{16}\right\rfloor+16\left(\left(3\left\lfloor\frac{i\bmod 16}{4}\right\rfloor+(i\bmod 4)\right)\bmod 4\right)+(i\bmod 4) \\
    P_{128}(i)&=4\left\lfloor\frac{i}{16}\right\rfloor+32\left(\left(3\left\lfloor\frac{i\bmod 16}{4}\right\rfloor+(i\bmod 4)\right)\bmod 4\right)+(i\bmod 4) \\
\end{align*}

\subsubsection{Round key addition}

The last step of each round consists in XORing a round key $R_i$ to the cipher
state. The new cipher state $s_{i+1}$ after each full round is therefore given
by

\[
    s_{i+1}=P(S(s_i))\oplus R_i
\]

\subsubsection{Round key extraction and key schedule}

Round key extraction differs for \texttt{GIFT-64} and \texttt{GIFT-128}. Let
$K=k_7||k_6||\dots||k_0$ denote the $128$-bit key state.

\paragraph{GIFT-64}. We extract two 16-bit words $U||V=k_1||k_0$ from the key
state. These are then added to the 64-bit round key: $R_{4i+1}\leftarrow
u_i,R_{4i}\leftarrow v_i$.

\paragraph{GIFT-128}. We extract two 32-bit words $U||V=k_5||k_4||k_1||k_0$ from
the key state. These are then added to the 128-bit round key: $R_{4i+2}\leftarrow
u_i,R_{4i+1}\leftarrow v_i$.

In both cases, we additionally XOR a round constant $C=c_5c_4c_3c_2c_1c_0$ to
bit positions $n-1,23,19,15,11,7,3$. The round constants are generated using a
6-bit affine linear-feedback shift register and have the following values:\\

\begin{tabular}{r|l}
    \textbf{Rounds} & \textbf{Constants} \\
    \hline
    \textbf{1 - 16} &  \small\texttt{01,03,07,0F,1F,3E,3D,3B,37,2F,1E,3C,39,33,27,0E} \\
    \textbf{17 - 32} & \small\texttt{1D,3A,35,2B,16,2C,18,30,21,02,05,0B,17,2E,1C,38} \\
    \textbf{33 - 48} & \small\texttt{31,23,06,0D,1B,36,2D,1A,34,29,12,24,08,11,22,04}
\end{tabular}\\

The key state is then updated by setting $k_1\leftarrow k_1\ggg 2$,
$k_0\leftarrow k_0\ggg 12$ and rotating the new state $32$ bits to the right:

\[
    k_7||k_6||\dots||k_1||k_0\leftarrow k_1\ggg 2||k_0\ggg 12||k_7||k_6||\dots||k_3||k_2
\]

\subsection{Camellia}

\section{The ARMv8 platform}

With small devices and embedded processors becoming ever more ubiquitous and
essential in areas like consumer electronics or industrial and IoT
applications, the need for low-power, high-performance microprocessors has
increased steadily. With more than 250 billion chips shipped, semiconductors
designed by ARM power 95\% of mobile devices and have found a great many
applications due to their high performance and low power consumption. The
ODROID-N2+\cite{odroidn2} development board we are using is based on the big.LITTLE
architecture and is powered by a quad-core ARM Cortex-A73 processor and a
weaker dual-core ARM Cortex-A53 for power efficiency. Both these processors are
part of the eight generation of ARM designs known as ARMv8\cite{armv8:2013}.

ARMv8 defines three architecture profiles for different use cases as well as
dynamic execution states with corresponding instruction sets. This work will
focus on the A profile running in the AArch64 state utilizing the A64
instruction set with NEON and crypto extensions.

\begin{table}[h!]
    \centering
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        Profile & Description \\
        \midrule
        Application (A) & Traditional use with virtual memory and privilege level support \\
        Real-time (R) & Real-time, low-latency, deterministic embedded systems \\
        Microcontroller (M) & Very low-power, fast-interrupt embedded systems \\
        \bottomrule
    \end{tabularx}
    \caption{ARMv8 profiles}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabularx}{\textwidth}{llX}
        \toprule
        Execution state & Usage & Instruction sets \\
        \midrule
        AArch32 & 32-bit compatibility & A32/T32 \\
        AArch64 & 64-bit & A64 \\
        \bottomrule
    \end{tabularx}
    \caption{ARMv8 execution states}
\end{table}

\subsection{General architecture}

ARMv8 is a RISC architecture employing simple data processing instructions
operating only on registers as well as dedicated load/store instructions to
transfer data from register to memory and back. This enables faster execution
of individual instructions, a simplier pipeline design, predictable instruction
timings and fewer addressing modes.

The A64 instruction set defines 31 64-bit general-purpose registers
\texttt{X0-X30} which can also be accessed as 32-bit registers \texttt{W0-W30}.
Values are loaded from and stored to memory using \texttt{LDR}/\texttt{STR}.
Data processing instructions generally use explicit output registers instead of
overwriting the first input register.

\begin{table}[h!]
    \centering
    \small
    \begin{tabularx}{\textwidth}{llX}
        \toprule
        Addressing mode & Example & Description \\
        \midrule
        Base register & \texttt{LDR W0, [X1]} & \texttt{W0 = *(X1);} \\
        Offset & \texttt{LDR W0, [X1, \#12]} & \texttt{W0 = *(X1 + 12);} \\
        Pre-indexing & \texttt{LDR W0, [X1, \#12]!} & \texttt{X1 += 12; W0 = *(X1)} \\
        Post-indexing & \texttt{LDR W0, [X1], \#12} & \texttt{W0 = *(X1); X1 += 12} \\
        \bottomrule
    \end{tabularx}
    \caption{AArch64 addressing modes}
\end{table}

\subsection{NEON}
\label{ss:neon}

ARMv8 supports single-instruction, multiple-data ARMv8 supports
single-instruction, multiple-data (SIMD) processing. These systems allow the
programmer to store multiple pieces of data in a vector and work on them in
parallel to speed up calculations. The A64 instruction set defines two possible
SIMD implementations.

\begin{enumerate}
    \item Advanced SIMD, known as NEON
    \item Scalable Vector Extension (SVE)
\end{enumerate}

We will take a look at NEON as this is the type of vector processing supported
by the Cortex-A73 processor.

The register file of the NEON unit is made up of 32 quad-word (128-bit)
registers \texttt{V[0-31]}, each extending the standard 64-bit floating-point
registers \mbox{\texttt{D[0-31]}}. These registers are divided into equally
sized lanes on which the vector instructions operate. Valid ways to interpret
for example the register \texttt{V0} are:

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figures/V_register.pdf}
    \caption{Divisions of the V0 register}
\end{figure}

NEON instructions interpret their operands' layouts (i.e. lane count and width)
through the use of suffixes such as \texttt{.4S} or \texttt{.8H}. For example,
adding eight 16-bit halfwords from register \texttt{V1} and \texttt{V2}
together and storing the result in \texttt{V0} can be done as follows:

\begin{center}
    \texttt{ADD V0.8H, V1.8H, V2.8H}
\end{center}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figures/vector_add.pdf}
    \caption{Addition of two vector registers}
\end{figure}

\subsection{NEON Intrinsics}

The header file \texttt{<arm\_neon.h>} provides ARM-specific data and function
definitions including vector data types and C functions for working with these
vectors. These functions are known as NEON intrinsics \cite{neonintr:2022} and
give the programmer a high-level interface to most NEON instructions. Major
advantages of this approach include the ease of development as the compiler
takes over register allocation and load/store operations as well as performance
benefits through compiler optimizations.

Standard vector data types have the format \texttt{uintnxm\_t} with lane width
$n$ in bits and and lane count $m$. Array types of the format
\texttt{uintnxmxc\_t}, $c\in\{2,3,4\}$ are also defined which are used in
operations requiring multiple parameters like \texttt{TBL} or pairwise
load/stores. Intrinsics include the operation name and lane data format as well
as an optional \texttt{q} suffix to indicate operation on a 128-bit register.
Multiplying eight pairs of 16-bit numbers \texttt{a,b} for example can be done
via the following:

\begin{center}
    \texttt{uint16x8\_t result = vmulq\_u16(a, b);}
\end{center}

In this case, the compiler allocates vector registers for \texttt{a},
\texttt{b} and \texttt{result} and assembles the intrinsic to \texttt{MUL
Vr.8H, Va.8H, Vb.8H}. Necessary loads and stores for the result and parameters
are also handled automatically. Of special interest to us are the following
intrinsics, each existing in different variants with different lane widths and
also array types: \\

\begin{table}[h!]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{llX}
        \toprule
        Intrinsic && Description \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{vreinterpretq\_u8\_u64(uint64x2\_t)} & Explicit casting \\
        \midrule
        \texttt{uint64\_t} & \texttt{vgetq\_lane\_u64(void)} & Extract a single lane \\
        \midrule
        \texttt{void} & \texttt{vsetq\_lane\_u64(uint64\_t)} & Insert a single lane \\
        \midrule
        \texttt{uint64x2\_t} & \texttt{vdupq\_n\_u64(uint64\_t)} & Initialize all lanes to same value \\
        \midrule
        \texttt{void} & \texttt{vst1q\_u64(uint64\_t*, uint64x2\_t)} & Store from register to memory \\
        \midrule
        \texttt{uint64x2\_t} & \texttt{vld1q\_u64(uint64\_t*, uint64x2\_t)} & Load from memory to register \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{veorq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise XOR \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{vandq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise AND \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{vorrq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise OR \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{vmvnq\_u8(uint8x16\_t)} & bitwise NOT \\
        \midrule
        \texttt{uint8x16\_t} & \texttt{vqtbl2q\_u8(uint8x16\_t, uint8x16\_t)} & permutation (\texttt{TBL}) \\
        \bottomrule
    \end{tabularx}
    \caption{Common NEON intrinsics}
\end{table}
