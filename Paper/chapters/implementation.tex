\chapter{Implementation}

We will provide and discuss implementations for the presented strategies in the
C programming language. Although directly writing Assembler code could result
in a small performance benefit, this generally increases the work necessary by
an order of magnitude for only limited results. Instruction-level optimization
and in particular register allocation is left to the compiler.

\section{Table-based}

We generate a 16 by 16 array of \texttt{uint64\_t} as explained in \ref{lst:tablesbox}.
The round function then consists of extracting each nibble, looking up the
corresponding table value and ORing this to the new cipher state.

\begin{lstlisting}[caption={Table-based round function}, language=c]
static const uint64_t table[16][16] = {
        { 0x0000000000000001UL, 0x0008000000020000UL,
          0x0000000400000000UL, 0x0008000400000000UL,
          0x0000000400020000UL, 0x0008000400020001UL,
          0x0000000000020001UL, 0x0008000000000001UL,
          0x0000000000020000UL, 0x0008000400000001UL,
          0x0008000000020001UL, 0x0000000400020001UL,
          0x0000000400000001UL, 0x0000000000000000UL,
          0x0008000000000000UL, 0x0008000400020000UL },
        // ...
        { 0x0000000010000000UL, 0x0000200000008000UL,
          0x4000000000000000UL, 0x4000000000008000UL,
          0x4000200000000000UL, 0x4000200010008000UL,
          0x0000200010000000UL, 0x0000000010008000UL,
          0x0000200000000000UL, 0x4000000010008000UL,
          0x0000200010008000UL, 0x4000200010000000UL,
          0x4000000010000000UL, 0x0000000000000000UL,
          0x0000000000008000UL, 0x4000200000008000UL }
};

uint64_t gift_64_table_subperm(const uint64_t cipher_state)
{
        uint64_t new_cipher_state = 0;

        for (size_t i = 0; i < 16; i++) {
                int nibble = (cipher_state >> (i * 4)) & 0xf;
                new_cipher_state ^= table[i][nibble];
        }

        return new_cipher_state;
}
\end{lstlisting}

The round function is similar for all strategies: for each round, we extract 16
bits from the key state which are then XORed to the round key, together with
the precomputed round constants and the single bit. Key state updating is done
using masks and shifts only which gets optimized by the compiler through
bit-field extraction instructions which can extract, shift and insert a bit
field at once. Intermediate values are also optimized away.

\begin{lstlisting}[caption={Round key generation function}, language=c]
void gift_64_table_generate_round_keys(uint64_t round_keys[restrict ROUNDS_GIFT_64],
                                       const uint64_t key[restrict 2])
{
        uint64_t key_state[] = {key[0], key[1]};
        for (int round = 0; round < ROUNDS_GIFT_64; round++) {
                int v = (key_state[0] >> 0 ) & 0xffff;
                int u = (key_state[0] >> 16) & 0xffff;

                // add round key (RK=U||V)
                round_keys[round] = 0UL;
                for (size_t i = 0; i < 16; i++) {
                        int key_bit_v   = (v >> i)  & 0x1;
                        int key_bit_u   = (u >> i)  & 0x1;
                        round_keys[round] ^= (uint64_t)key_bit_v << (i * 4 + 0);
                        round_keys[round] ^= (uint64_t)key_bit_u << (i * 4 + 1);
                }

                // add single bit
                round_keys[round] ^= 1UL << 63;

                // add round constants
                round_keys[round] ^= ((round_constant[round] >> 0) & 0x1) << 3;
                round_keys[round] ^= ((round_constant[round] >> 1) & 0x1) << 7;
                round_keys[round] ^= ((round_constant[round] >> 2) & 0x1) << 11;
                round_keys[round] ^= ((round_constant[round] >> 3) & 0x1) << 15;
                round_keys[round] ^= ((round_constant[round] >> 4) & 0x1) << 19;
                round_keys[round] ^= ((round_constant[round] >> 5) & 0x1) << 23;

                // update key state
                int k0 = (key_state[0] >> 0 ) & 0xffffUL;
                int k1 = (key_state[0] >> 16) & 0xffffUL;
                k0 = (k0 >> 12) | ((k0 & 0xfff) << 4);
                k1 = (k1 >> 2 ) | ((k1 & 0x3  ) << 14);
                key_state[0] >>= 32;
                key_state[0] |= (key_state[1] & 0xffffffffUL) << 32;
                key_state[1] >>= 32;
                key_state[1] |= ((uint64_t)k0 << 32) | ((uint64_t)k1 << 48);
        }
}
\end{lstlisting}

While this approach does not rely on ARM-specific instructions, the next two
strategies do. This is why we use NEON intrinsics.

\section{Using \texttt{vperm}}

\subsection{NEON intrinsics}

The header file \texttt{<arm\_neon.h>} provides ARM-specific data and function
definitions including vector data types and C functions for working with these
vectors. These functions are known as NEON intrinsics and give the programmer a
high-level interface to most NEON instructions. Major advantages of this
approach include the ease of development as the compiler takes over register
allocation and load/store operations as well as performance benefits through
compiler optimizations.

Standard vector data types have the format \texttt{uintnxm\_t} with lane width
$n$ in bits and and lane count $m$. Array types of the format
\texttt{uintnxmxc\_t}, $c\in\{2,3,4\}$ are also defined which are used in
operations requiring multiple parameters like \texttt{TBL} or pairwise
load/stores. Intrinsics start with \texttt{v}, optionally contain a \texttt{q}
after the operation name to indicate operation on a 128-bit register, and end
with the lane data format. Multiplying eight pairs of 16-bit numbers
\texttt{a,b} for example can be done via the following:

\begin{center}
    \texttt{uint16x8\_t result = vmulq\_u16(a, b);}
\end{center}

In this case, the compiler allocates vector registers for \texttt{a},
\texttt{b} and \texttt{result} and assembles the intrinsic to \texttt{MUL
Vr.8H, Va.8H, Vb.8H}. Necessary loads and stores for the result and parameters
are also handled automatically. Of special interest to us are the following
intrinsics, each existing in different variants with different lane widths and
also array types: \\

\begin{table}[h!]
    \centering
    \footnotesize
    \begin{tabularx}{\textwidth}{ll|X}
        Intrinsic && Description \\
        \hline
        \texttt{uint8x16\_t} & \texttt{vreinterpretq\_u8\_u64(uint64x2\_t)} & Explicit casting \\
        \hline
        \texttt{uint64\_t} & \texttt{vgetq\_lane\_u64(void)} & Extract a single lane \\
        \hline
        \texttt{void} & \texttt{vsetq\_lane\_u64(uint64\_t)} & Insert a single lane \\
        \hline
        \texttt{uint64x2\_t} & \texttt{vdupq\_n\_u64(uint64\_t)} & Initialize all lanes to same value \\
        \hline
        \texttt{void} & \texttt{vst1q\_u64(uint64\_t*, uint64x2\_t)} & Store from register to memory \\
        \hline
        \texttt{uint64x2\_t} & \texttt{vld1q\_u64(uint64\_t*, uint64x2\_t)} & Load from memory to register \\
        \hline
        \texttt{uint8x16\_t} & \texttt{veorq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise XOR \\
        \hline
        \texttt{uint8x16\_t} & \texttt{vandq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise AND \\
        \hline
        \texttt{uint8x16\_t} & \texttt{vorrq\_u8(uint8x16\_t, uint8x16\_t)} & bitwise OR \\
        \hline
        \texttt{uint8x16\_t} & \texttt{vmvnq\_u8(uint8x16\_t)} & bitwise NOT \\
        \hline
        \texttt{uint8x16\_t} & \texttt{vqtbl2q\_u8(uint8x16\_t, uint8x16\_t)} & permutation (\texttt{TBL}) \\
    \end{tabularx}
\end{table}

\section{Bitslicing}

